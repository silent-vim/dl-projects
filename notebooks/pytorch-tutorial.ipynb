{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e-37 *\n",
      "  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  6.0323\n",
      "  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0328  0.0702  0.8248\n",
      " 0.7820  0.1193  0.2764\n",
      " 0.6061  0.7468  0.9306\n",
      " 0.4257  0.2542  0.9128\n",
      " 0.1363  0.5086  0.5652\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.4430  0.5790  1.7164\n",
      " 1.2225  0.2501  0.6272\n",
      " 1.5923  1.7097  1.6964\n",
      " 0.4633  0.7913  1.0858\n",
      " 0.4289  1.1314  1.5247\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.4430  0.5790  1.7164\n",
      " 1.2225  0.2501  0.6272\n",
      " 1.5923  1.7097  1.6964\n",
      " 0.4633  0.7913  1.0858\n",
      " 0.4289  1.1314  1.5247\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.4430  0.5790  1.7164\n",
       " 1.2225  0.2501  0.6272\n",
       " 1.5923  1.7097  1.6964\n",
       " 0.4633  0.7913  1.0858\n",
       " 0.4289  1.1314  1.5247\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.4758  0.6492  2.5412\n",
      " 2.0045  0.3693  0.9036\n",
      " 2.1983  2.4565  2.6270\n",
      " 0.8889  1.0456  1.9987\n",
      " 0.5652  1.6400  2.0899\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print (x)\n",
    "y = x + 2\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd._functions.basic_ops.AddConstant object at 0x10a0206a8>\n"
     ]
    }
   ],
   "source": [
    "print (y.creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      " Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print (z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print (x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear (400 -> 120)\n",
      "  (fc2): Linear (120 -> 84)\n",
      "  (fc3): Linear (84 -> 10)\n",
      ")\n",
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "params = list(net.parameters())\n",
    "print (len(params))\n",
    "print (params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.1493  0.0935  0.0308 -0.0170  0.1062  0.0022  0.0813 -0.0436 -0.0444 -0.1259\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(1, 1, 32, 32))\n",
    "out = net(input)\n",
    "print (out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 38.6734\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = Variable(torch.arange(1, 11))\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "print (loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.nn._functions.thnn.auto.MSELoss object at 0x10d1ea220>\n",
      "<torch.nn._functions.linear.Linear object at 0x10d1ea138>\n"
     ]
    }
   ],
   "source": [
    "print (loss.creator)\n",
    "print (loss.creator.previous_functions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 6]\n",
      "\n",
      "Variable containing:\n",
      "-0.0281\n",
      " 0.0388\n",
      " 0.1090\n",
      " 0.0315\n",
      "-0.0624\n",
      "-0.0877\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "print(net.conv1.bias.grad)\n",
    "loss.backward()\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "optimizer.zero_grad()\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Deep Learning for NLP Tutorial\n",
    "http://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#sphx-glr-beginner-nlp-deep-learning-tutorial-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'en': 3, u'No': 9, u'buena': 14, u'it': 7, u'at': 22, u'sea': 12, u'cafeteria': 5, u'Yo': 23, u'la': 4, u'to': 8, u'creo': 10, u'is': 16, u'a': 18, u'good': 19, u'get': 20, u'idea': 15, u'que': 11, u'not': 17, u'me': 0, u'on': 25, u'gusta': 1, u'lost': 21, u'Give': 6, u'una': 13, u'si': 24, u'comer': 2}\n",
      "Vocabulary Size -  26\n",
      "Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.1385  0.1573  0.0351 -0.0379  0.0751  0.0261  0.0070  0.1680  0.0383  0.0158\n",
      " 0.1227  0.0064 -0.0273 -0.0368 -0.0761  0.0403 -0.0231 -0.1456 -0.1463  0.1173\n",
      "\n",
      "Columns 10 to 19 \n",
      "-0.0881  0.1685 -0.1469  0.1335  0.0034  0.0788  0.0019  0.1575 -0.0013 -0.0349\n",
      " 0.0657 -0.0799  0.0680  0.1200 -0.0727 -0.0277  0.0082 -0.0342  0.0218  0.1017\n",
      "\n",
      "Columns 20 to 25 \n",
      " 0.0984 -0.0435 -0.0021 -0.0577  0.0987  0.1188\n",
      "-0.1718 -0.1375  0.0579  0.0388 -0.0869 -0.1880\n",
      "[torch.FloatTensor of size 2x26]\n",
      "\n",
      "Parameter containing:\n",
      " 0.1668\n",
      "-0.1391\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "Variable containing:\n",
      "-0.5154 -0.9095\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-0.4779 -0.9677\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-0.3859 -1.1389\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "*******Before the training*********\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      " -8.8071\n",
      "  6.5665\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "***********************************\n",
      "Variable containing:\n",
      "-0.2309 -1.5788\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-1.3660 -0.2945\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "*******After the training*********\n",
      "Variable containing:\n",
      " 0.1388\n",
      "-0.1612\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "***********************************\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "data = [\n",
    "    (\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "    (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "    (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "    (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")\n",
    "]\n",
    "\n",
    "test_data = [\n",
    "    (\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "    (\"it is lost on me\".split(), \"ENGLISH\")\n",
    "]\n",
    "\n",
    "label_to_idx = {\"SPANISH\" : 0, \"ENGLISH\" : 1}\n",
    "\n",
    "# word_to_idx maps each word to an unique integer which will be its index \n",
    "# into the bag of words vector\n",
    "\n",
    "word_to_idx = {}\n",
    "for sentence, _ in data + test_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "print (word_to_idx)\n",
    "VOCAB_SIZE = len(word_to_idx)\n",
    "NUM_LABELS = 2 # English and Spanish represented by two labels\n",
    "\n",
    "class BoWClassifier(nn.Module): # inheriting from the nn.Module\n",
    "    \n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # call the init function of the nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        \n",
    "        # define the parameters that will be needed\n",
    "        # In this case we need A and b parameters of the affine mapping.\n",
    "        \n",
    "        # vocab_size = size of input sample\n",
    "        # num_labels = size of output sample\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "        \n",
    "    def forward(self, bow_vec):\n",
    "        return F.log_softmax(self.linear(bow_vec))\n",
    "\n",
    "\n",
    "def make_bow_vector(sentence, word_to_idx):\n",
    "    vec = torch.zeros(len(word_to_idx))\n",
    "    for word in sentence:\n",
    "        vec[word_to_idx[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "\n",
    "def make_target(label, label_to_idx):\n",
    "    return torch.LongTensor([label_to_idx[label]])\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "print (\"Vocabulary Size - \", VOCAB_SIZE)\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "    \n",
    "sample = data[0]\n",
    "bow_vector = make_bow_vector(sample[0], word_to_idx)\n",
    "log_probs = model(Variable(bow_vector))\n",
    "print (log_probs)\n",
    "\n",
    "# run on test data before we train, to see before and after\n",
    "for instance, label in test_data:\n",
    "    bow_vec = Variable(make_bow_vector(instance, word_to_idx))\n",
    "    log_probs = model(bow_vec)\n",
    "    print (log_probs)\n",
    "    \n",
    "# print the matrix column corresponding to \"creo\"\n",
    "print (\"*******Before the training*********\")\n",
    "print (next(model.parameters())[:, word_to_idx[\"creo\"]])\n",
    "print (\"***********************************\")\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# pass over the training data several times \n",
    "# 100 is much bigger on a real dataset usually 5-30 epochs is resonable\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        # step 1. pytorch accumulates gradients, we need to clear \n",
    "        # them out before each instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # step 2. make our bow vector and also we must wrap the target\n",
    "        # in a Variable as an integer. For example if the target is \n",
    "        # SPANISH, then we wrap the integer 0. The loss function then \n",
    "        # knows that the 0th element of the log probabilities is the log \n",
    "        # probability corresponding to SPANISH\n",
    "        bow_vec = Variable(make_bow_vector(instance, word_to_idx))\n",
    "        target = Variable(make_target(label, label_to_idx))\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        log_probs = model(bow_vec)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "for instance, label in test_data:\n",
    "    bow_vec = Variable(make_bow_vector(instance, word_to_idx))\n",
    "    log_probs = model(bow_vec)\n",
    "    print (log_probs)\n",
    "    \n",
    "# Index corresponding to spanish goes up, english goes down\n",
    "print (\"*******After the training*********\")\n",
    "print (next(model.parameters())[:, word_to_idx[\"creo\"]])\n",
    "print (\"***********************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
